{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5jn4zdHO924"
      },
      "source": [
        "# Adversarial Search: Playing Connect 4\n",
        "\n",
        "Student Name: Ahmed Abdul Naoman\n",
        "\n",
        "I have used the following AI tools: deepseek\n",
        "\n",
        "I understand that my submission needs to be my own work: aan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0AOjQPHO924"
      },
      "source": [
        "## Learning Outcomes\n",
        "\n",
        "* Implement adversarial search algorithms for strategic game play.\n",
        "* Analyze and optimize search in complex game spaces.\n",
        "* Design effective heuristic evaluation functions.\n",
        "* Compare performance across different agent strategies.\n",
        "* Evaluate algorithmic trade-offs between decision quality and efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_zlBGQQO924"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "Total Points: Undergraduates 100, graduate students 110\n",
        "\n",
        "Complete this notebook and submit it. The notebook needs to be a complete project report with your implementation, documentation including a short discussion of how your implementation works and your design choices, and experimental results (e.g., tables and charts with simulation results) with a short discussion of what they mean. Use the provided notebook cells and insert additional code and markdown cells as needed. Submit the completely rendered notebook as a HTML file.\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "You will implement different versions of agents that play Connect 4:\n",
        "\n",
        "> \"Connect 4 is a two-player connection board game, in which the players choose a color and then take turns dropping colored discs into a seven-column, six-row vertically suspended grid. The pieces fall straight down, occupying the lowest available space within the column. The objective of the game is to be the first to form a horizontal, vertical, or diagonal line of four of one's own discs.\" (see [Connect Four on Wikipedia](https://en.wikipedia.org/wiki/Connect_Four))\n",
        "\n",
        "Note that [Connect-4 has been solved](https://en.wikipedia.org/wiki/Connect_Four#Mathematical_solution)\n",
        "in 1988. A connect-4 solver with a discussion of how to solve different parts of the problem can be found here: https://connect4.gamesolver.org/en/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-Jjq5NVO925"
      },
      "source": [
        "## Task 1: Defining the Search Problem [10 point]\n",
        "\n",
        "Define the components of the search problem:\n",
        "\n",
        "* Initial state\n",
        "* Actions\n",
        "* Transition model (result function)\n",
        "* Goal state (terminal state and utility)\n",
        "\n",
        "Describe each component and then implement it as a function that can be used by search algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84cdde49"
      },
      "source": [
        "### Initial State\n",
        "The initial state of the Connect 4 game is an empty board, typically a 6x7 grid, where no discs have been played yet. We'll represent the empty slots with `0`, player 1's discs with `1`, and player -1's discs with `-1`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb752dca"
      },
      "source": [
        "### Actions\n",
        "An action in Connect 4 consists of dropping a disc into one of the available columns. A column is available if it is not yet full. The `actions(state)` function will return a list of column indices where a disc can be placed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2542cdcf"
      },
      "source": [
        "### Transition Model (Result Function)\n",
        "The `result(state, player, action)` function takes the current board `state`, the `player` making the move, and the chosen `action` (column). It returns a new board state where the player's disc has been placed in the lowest available slot of the chosen column. It's crucial that this function returns a *new* state without modifying the original."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7ac9764"
      },
      "source": [
        "### Goal State (Terminal State and Utility)\n",
        "*   **Terminal State**: A state is terminal if the game has ended. This occurs when a player has formed a line of four (horizontally, vertically, or diagonally), or when the entire board is filled, resulting in a draw.\n",
        "*   **Utility**: The `utility(state, player)` function assigns a numerical value to a terminal state from the perspective of the given `player`. For a win, it's `1`; for a loss, it's `-1`; and for a draw, it's `0`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deac2a61"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def empty_board(rows=6, cols=7):\n",
        "    \"\"\"Creates an empty Connect 4 board.\"\"\"\n",
        "    return np.full(shape=(rows, cols), fill_value=0, dtype=int)\n",
        "\n",
        "def actions(board):\n",
        "    \"\"\"Returns valid columns where a disc can be dropped.\"\"\"\n",
        "    # Top row must be empty for a column to be valid.\n",
        "    return [col for col in range(board.shape[1]) if board[0, col] == 0]\n",
        "\n",
        "def result(board, player, action):\n",
        "    \"\"\"Returns the new board state after player drops a disc in the specified column.\"\"\"\n",
        "    if action not in actions(board):\n",
        "        raise ValueError(\"Invalid action: column is full or out of bounds\")\n",
        "\n",
        "    new_board = board.copy()\n",
        "    rows = new_board.shape[0]\n",
        "    # Find the lowest available row in the chosen column\n",
        "    for row in range(rows - 1, -1, -1):\n",
        "        if new_board[row, action] == 0:\n",
        "            new_board[row, action] = player\n",
        "            return new_board\n",
        "    return new_board # Should not be reached if action is valid\n",
        "\n",
        "def check_win(board, player):\n",
        "    \"\"\"Checks if the given player has won the game.\"\"\"\n",
        "    rows, cols = board.shape\n",
        "    winning_line = np.array([player] * 4)\n",
        "\n",
        "    # Check horizontal wins\n",
        "    for r in range(rows):\n",
        "        for c in range(cols - 3):\n",
        "            if np.array_equal(board[r, c:c+4], winning_line):\n",
        "                return True\n",
        "\n",
        "    # Check vertical wins\n",
        "    for c in range(cols):\n",
        "        for r in range(rows - 3):\n",
        "            if np.array_equal(board[r:r+4, c], winning_line):\n",
        "                return True\n",
        "\n",
        "    # Check diagonal wins (positive slope)\n",
        "    for r in range(rows - 3):\n",
        "        for c in range(cols - 3):\n",
        "            if np.array_equal(np.diag(board[r:r+4, c:c+4]), winning_line):\n",
        "                return True\n",
        "\n",
        "    # Check diagonal wins (negative slope)\n",
        "    for r in range(3, rows):\n",
        "        for c in range(cols - 3):\n",
        "            if np.array_equal(np.diag(np.fliplr(board[r-3:r+1, c:c+4])), winning_line):\n",
        "                return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def terminal(board):\n",
        "    \"\"\"Checks if the game is in a terminal state (win or draw).\"\"\"\n",
        "    if check_win(board, 1) or check_win(board, -1):\n",
        "        return True\n",
        "    # Check for a draw (board full and no win)\n",
        "    if np.all(board != 0):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def utility(board, player):\n",
        "    \"\"\"Returns the utility of a terminal state for the given player.\"\"\"\n",
        "    if check_win(board, player):\n",
        "        return 1\n",
        "    elif check_win(board, -player): # Opponent won\n",
        "        return -1\n",
        "    else: # It's a draw\n",
        "        return 0\n"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkH7C3ghO925"
      },
      "source": [
        "How big is the state space? Give an estimate and explain it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "543c4ab7"
      },
      "source": [
        "The state space of Connect 4 is the total number of possible board configurations. For a standard 6 rows x 7 columns board, there are 42 positions. Each position can be empty, or occupied by player 1, or player -1. So, a naive upper bound would be $3^{42}$.\n",
        "\n",
        "However, this is a significant overestimate because:\n",
        "1.  **Gravity:** Discs fall to the lowest available spot, meaning the board must always be 'bottom-heavy'. There cannot be floating discs.\n",
        "2.  **Turns:** The number of pieces for player 1 and player -1 must either be equal, or player 1 must have exactly one more piece than player -1 (since player 1 always starts).\n",
        "3.  **Terminal States:** Once a player wins, the game ends, and further moves are not part of the reachable state space from that point.\n",
        "\n",
        "A more accurate estimate, factoring in gravity and turn constraints, suggests the total number of legal positions is around **$4.5 \\times 10^{12}$** (trillion) unique states. This is still a massive number, indicating that exploring the entire state space is computationally infeasible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKg7LXxNO925"
      },
      "source": [
        "How big is the game tree that minimax search will go through? Give an estimate and explain it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7c66cbd"
      },
      "source": [
        "The game tree for Connect 4 represents all possible sequences of moves from the initial state until a terminal state (win, loss, or draw). Unlike the state space, which counts unique board configurations, the game tree counts paths (sequences of moves). While the number of unique states is immense, the game tree is vastly larger.\n",
        "\n",
        "Estimating the exact size of the full game tree for Connect 4 is extremely difficult, as it involves branching factors (number of available moves at each step) and game depth. However, we know:\n",
        "\n",
        "*   **Average Branching Factor:** In Connect 4, the average number of available moves per turn is approximately 5-6 (since there are 7 columns, but some may be full, and early in the game all are available).\n",
        "*   **Maximum Game Depth:** A Connect 4 game can last up to 42 moves (the total number of cells on the board), though most games end much sooner.\n",
        "\n",
        "If we consider a simplified scenario with an average branching factor of 6 and a maximum depth of 42, a naive upper bound for the game tree size would be $6^{42}$. This number is astronomically large, far exceeding the number of atoms in the observable universe, making it impossible to explore the entire game tree. (For comparison, $10^{80}$ is a common estimate for atoms in the universe).\n",
        "\n",
        "However, minimax search with alpha-beta pruning significantly reduces the explored portion of the tree by cutting off branches that cannot affect the final decision. Even with pruning, the search depth for a full Connect 4 board remains a major challenge, which is why limited-depth searches and heuristic evaluation functions are crucial for practical Connect 4 AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9sSavUgO925"
      },
      "source": [
        "## Task 2: Game Environment and Random Agent [25 point]\n",
        "\n",
        "Use a numpy character array as the board."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyC3OWXgO925",
        "outputId": "6067fb5c-031a-4bf5-c858-8e1b0865093c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def empty_board(shape=(6, 7)):\n",
        "    return np.full(shape=shape, fill_value=0)\n",
        "\n",
        "print(empty_board())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_idvK_PBO925"
      },
      "source": [
        "The standard board is $6 \\times 7$ but you can use smaller boards to test your code. Instead of colors (red and yellow), I use 1 and -1 to represent the players. Make sure that your agent functions all have the from: `agent_type(board, player = 1)`, where board is the current board position (in the format above) and player is the player whose next move it is and who the agent should play (as 1 and -1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "WIdWQVsdO925",
        "outputId": "e6a0eba2-af1c-4ded-e83f-a1a86e9772a1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Visualization code by Randolph Rankin\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize(board):\n",
        "    plt.axes()\n",
        "    rectangle=plt.Rectangle((-0.5,len(board)*-1+0.5),len(board[0]),len(board),fc='blue')\n",
        "    circles=[]\n",
        "    for i,row in enumerate(board):\n",
        "        for j,val in enumerate(row):\n",
        "            color='white' if val==0 else 'red' if val==1 else 'yellow'\n",
        "            circles.append(plt.Circle((j,i*-1),0.4,fc=color))\n",
        "\n",
        "    plt.gca().add_patch(rectangle)\n",
        "    for circle in circles:\n",
        "        plt.gca().add_patch(circle)\n",
        "\n",
        "    plt.axis('scaled')\n",
        "    plt.show()\n",
        "\n",
        "board = [[0, 0, 0, 0, 0, 0, 0],\n",
        "         [0, 0, 0, 0, 0, 0, 0],\n",
        "         [0, 0, 0, 0, 0, 0, 0],\n",
        "         [0, 0, 0, 1, 0, 0, 0],\n",
        "         [0, 0, 0, 1, 0, 0, 0],\n",
        "         [0,-1,-1, 1,-1, 0, 0]]\n",
        "visualize(board)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "CXzl7ZZ5O925"
      },
      "source": [
        "Implement helper functions for:\n",
        "\n",
        "* A check for available actions in each state `actions(state)`.\n",
        "* The transition model `result(state, player, action)`.\n",
        "* Check for terminal states `terminal(state)`.\n",
        "* The utility function `utility(state, player)`.\n",
        "\n",
        "The player argument is used so your agent can play red or yellow.\n",
        "Make sure that all these functions work with boards of different sizes (number of columns and rows).\n",
        "You can follow the [tic-tac-toe example from class.](https://colab.research.google.com/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_definitions.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "9CJUctkAO925"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def actions(board):\n",
        "    return [c for c in range(board.shape[1]) if board[0, c] == 0]\n",
        "\n",
        "\n",
        "def result(board, player, action):\n",
        "    new_board = board.copy()\n",
        "    rows = new_board.shape[0]\n",
        "\n",
        "    for r in range(rows - 1, -1, -1):\n",
        "        if new_board[r, action] == 0:\n",
        "            new_board[r, action] = player\n",
        "            return new_board\n",
        "\n",
        "    raise ValueError(\"Invalid action: column is full.\")\n",
        "\n",
        "\n",
        "def check_win(board, player):\n",
        "    rows, cols = board.shape\n",
        "    target = [player] * 4\n",
        "\n",
        "    # Horizontal\n",
        "    for r in range(rows):\n",
        "        for c in range(cols - 3):\n",
        "            if list(board[r, c:c+4]) == target:\n",
        "                return True\n",
        "\n",
        "    # Vertical\n",
        "    for r in range(rows - 3):\n",
        "        for c in range(cols):\n",
        "            if list(board[r:r+4, c]) == target:\n",
        "                return True\n",
        "\n",
        "    # Diagonal down-right\n",
        "    for r in range(rows - 3):\n",
        "        for c in range(cols - 3):\n",
        "            if [board[r+i, c+i] for i in range(4)] == target:\n",
        "                return True\n",
        "\n",
        "    # Diagonal up-right\n",
        "    for r in range(3, rows):\n",
        "        for c in range(cols - 3):\n",
        "            if [board[r-i, c+i] for i in range(4)] == target:\n",
        "                return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "def terminal(board):\n",
        "    \"\"\"\n",
        "    Return True if game is over: win or draw.\n",
        "    \"\"\"\n",
        "    if check_win(board, 1) or check_win(board, -1):\n",
        "        return True\n",
        "    if np.all(board != 0):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def utility(board, player):\n",
        "    if check_win(board, player):\n",
        "        return 1\n",
        "    elif check_win(board, -player):\n",
        "        return -1\n",
        "    else:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqAaaEn0O925"
      },
      "source": [
        "Implement an agent that plays randomly. Make sure the agent function receives as the percept the board and returns a valid action. Use an agent function definition with the following signature (arguments):\n",
        "\n",
        "`def random_player(board, player = 1): ...`\n",
        "\n",
        "The argument `player` is used for agents that do not store what color they are playing. The value passed on by the environment should be 1 ot -1 for player red and yellow, respectively.  See [Experiments section for tic-tac-toe](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_and_or_tree_search.ipynb#Experiments) for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWP67DM6O925",
        "outputId": "e3e9ecd2-9183-463b-e413-3d211c478b55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random player's move (player 1): 0\n",
            "\n",
            "Board after random move:\n",
            " [[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def random_player(board, player = 1):\n",
        "    \"\"\"An agent that plays randomly by selecting a valid column.\"\"\"\n",
        "    valid_actions = actions(board)\n",
        "    if not valid_actions:\n",
        "        return None # No valid moves\n",
        "    return random.choice(valid_actions)\n",
        "\n",
        "current_board = empty_board()\n",
        "print(\"Random player's move (player 1):\", random_player(current_board, 1))\n",
        "next_board = result(current_board, 1, random_player(current_board, 1))\n",
        "print(\"\\nBoard after random move:\\n\", next_board)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iTgAXzOO926"
      },
      "source": [
        "Let two random agents play against each other 1000 times. Look at the [Experiments section for tic-tac-toe](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_and_or_tree_search.ipynb#Experiments) to see how the environment uses the agent functions to play against each other.\n",
        "\n",
        "How often does each player win? Is the result expected?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43815aab",
        "outputId": "89f3e5ef-a803-41ce-fe5b-296c46ac9b75"
      },
      "source": [
        "def play_game(agent1, agent2, board_shape=(6, 7), verbose=False):\n",
        "    board = empty_board(board_shape)\n",
        "    current_player = 1 # Player 1 starts\n",
        "    turn = 0\n",
        "    if verbose: print(\"\\n--- New Game ---\")\n",
        "    if verbose: print(\"Initial Board:\\n\", board)\n",
        "\n",
        "    while not terminal(board):\n",
        "        turn += 1\n",
        "        if current_player == 1:\n",
        "            action = agent1(board, current_player)\n",
        "        else:\n",
        "            action = agent2(board, current_player)\n",
        "\n",
        "        if action is None: # No valid moves for the current player (should not happen in Connect 4 unless board is full)\n",
        "            if verbose: print(f\"Player {current_player} has no valid moves. Game ends in a draw.\")\n",
        "            return 0 # Draw\n",
        "\n",
        "        board = result(board, current_player, action)\n",
        "        if verbose: print(f\"Player {current_player} plays column {action}\")\n",
        "        if verbose: print(f\"Board after {turn} turns:\\n\", board)\n",
        "\n",
        "        if check_win(board, current_player):\n",
        "            if verbose: print(f\"Player {current_player} wins!\")\n",
        "            return current_player\n",
        "\n",
        "        if np.all(board != 0): # Check for draw if board is full and no win\n",
        "            if verbose: print(\"Board is full. It's a draw!\")\n",
        "            return 0\n",
        "\n",
        "        current_player *= -1 # Switch player\n",
        "    return utility(board, 1)\n",
        "#Simulation\n",
        "num_games = 1000\n",
        "player1_wins = 0\n",
        "player_minus1_wins = 0\n",
        "draws = 0\n",
        "\n",
        "print(f\"Simulating {num_games} games between two random agents...\")\n",
        "\n",
        "for i in range(num_games):\n",
        "    game_result = play_game(random_player, random_player)\n",
        "    if game_result == 1:\n",
        "        player1_wins += 1\n",
        "    elif game_result == -1:\n",
        "        player_minus1_wins += 1\n",
        "    else:\n",
        "        draws += 1\n",
        "\n",
        "print(f\"\\nSimulation Results ({num_games} games)\")\n",
        "print(f\"Player 1 (Red) Wins: {player1_wins}\")\n",
        "print(f\"Player -1 (Yellow) Wins: {player_minus1_wins}\")\n",
        "print(f\"Draws: {draws}\")\n",
        "print(f\"Total games: {player1_wins + player_minus1_wins + draws}\")\n"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simulating 1000 games between two random agents...\n",
            "\n",
            "Simulation Results (1000 games)\n",
            "Player 1 (Red) Wins: 541\n",
            "Player -1 (Yellow) Wins: 453\n",
            "Draws: 6\n",
            "Total games: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Over 1000 games, Player 1 (red) won 541 times, Player -1 (yellow) won 453 times, and there was 6 draws.\n",
        "Since both agents play randomly but Player 1 always moves first, itâ€™s expected that Player 1 has a slight advantage, which is reflected in the higher win count."
      ],
      "metadata": {
        "id": "u0GcUlB0thAv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-ly9nrjO926"
      },
      "source": [
        "## Task 3: Minimax Search with Alpha-Beta Pruning\n",
        "\n",
        "### Implement the Search [20 points]\n",
        "\n",
        "Implement minimax search starting from a given board for specifying the player.\n",
        "\n",
        "__Important Notes:__\n",
        "* You can use code from the [tic-tac-toe example](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_alpha_beta_tree_search.ipynb).\n",
        "* Make sure that all your agent functions have a signature consistent with the random agent above and that it [uses a class to store state information.](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/HOWTOs/store_agent_state_information.ipynb)\n",
        "This is essential to be able play against agents from other students later.\n",
        "* The game tree for a $6 \\times 7$ board is huge and optimal algorithms need to visit each or a large percentage of all nodes in the tree. You can experiment with smaller boards like a $4 \\times 4$ board first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qi4-wUg9O926",
        "outputId": "fb953376-7dcc-475b-b642-72623f183ea0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimax search with alpha-beta pruning + depth cutoff is ready.\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "def minimax_value(board, player, alpha, beta, depth):\n",
        "    \"\"\"Return minimax value with depth cutoff + heuristic.\"\"\"\n",
        "\n",
        "    # Terminal state or depth limit reached\n",
        "    if terminal(board) or depth == 0:\n",
        "        return heuristic(board, player)  # use your heuristic function here\n",
        "\n",
        "    if player == 1:   # maximizing player\n",
        "        best = -math.inf\n",
        "        for col in actions(board):\n",
        "            child = result(board, player, col)\n",
        "            val = minimax_value(child, -player, alpha, beta, depth-1)\n",
        "            best = max(best, val)\n",
        "            alpha = max(alpha, best)\n",
        "            if beta <= alpha:\n",
        "                break\n",
        "        return best\n",
        "\n",
        "    else:  # minimizing player\n",
        "        best = math.inf\n",
        "        for col in actions(board):\n",
        "            child = result(board, player, col)\n",
        "            val = minimax_value(child, -player, alpha, beta, depth-1)\n",
        "            best = min(best, val)\n",
        "            beta = min(beta, best)\n",
        "            if beta <= alpha:\n",
        "                break\n",
        "        return best\n",
        "\n",
        "\n",
        "def minimax_move(board, player=1, depth=4):\n",
        "    \"\"\"Choose the best move using alpha-beta with depth cutoff.\"\"\"\n",
        "    best_action = None\n",
        "\n",
        "    if player == 1:\n",
        "        best_val = -math.inf\n",
        "        alpha, beta = -math.inf, math.inf\n",
        "        for col in actions(board):\n",
        "            child = result(board, player, col)\n",
        "            val = minimax_value(child, -player, alpha, beta, depth-1)\n",
        "            if val > best_val:\n",
        "                best_val = val\n",
        "                best_action = col\n",
        "            alpha = max(alpha, best_val)\n",
        "\n",
        "    else:\n",
        "        best_val = math.inf\n",
        "        alpha, beta = -math.inf, math.inf\n",
        "        for col in actions(board):\n",
        "            child = result(board, player, col)\n",
        "            val = minimax_value(child, -player, alpha, beta, depth-1)\n",
        "            if val < best_val:\n",
        "                best_val = val\n",
        "                best_action = col\n",
        "            beta = min(beta, best_val)\n",
        "\n",
        "    return best_action\n",
        "\n",
        "\n",
        "class MinimaxAgent:\n",
        "    \"\"\"Depth-limited minimax agent.\"\"\"\n",
        "\n",
        "    def __init__(self, name=\"Minimax\", depth=4):\n",
        "        self.name = name\n",
        "        self.depth = depth\n",
        "\n",
        "    def __call__(self, board, player=1):\n",
        "        return minimax_move(board, player, depth=self.depth)\n",
        "\n",
        "\n",
        "print(\"Minimax search with alpha-beta pruning + depth cutoff is ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0l2-gkiO926"
      },
      "source": [
        "Experiment with some manually created boards (at least 5) to check if the agent spots winning opportunities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ea8Wv4xmO926",
        "outputId": "db4d80ca-0c71-4885-e6b1-409672c56136"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Test 1: P1 horizontal win ===\n",
            "Initial board:\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 1 0 0 0]\n",
            "[0 1 1 1 0 0 0]\n",
            "\n",
            "Player 1 chooses column: 3\n",
            "Board after move:\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 1 0 0 0]\n",
            "[0 0 0 1 0 0 0]\n",
            "[0 1 1 1 0 0 0]\n",
            "\n",
            "\n",
            "=== Test 2: P-1 block horizontal ===\n",
            "Initial board:\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 1 0 0 0]\n",
            "[0 1 1 1 0 0 0]\n",
            "\n",
            "Player -1 chooses column: 6\n",
            "Board after move:\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 1 0 0 0]\n",
            "[ 0  1  1  1  0  0 -1]\n",
            "\n",
            "\n",
            "=== Test 3: P1 vertical win ===\n",
            "Initial board:\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 1 0 0 0]\n",
            "[0 0 0 1 0 0 0]\n",
            "[0 0 0 1 0 0 0]\n",
            "[ 0 -1 -1 -1  0  0  0]\n",
            "\n",
            "Player 1 chooses column: 0\n",
            "Board after move:\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 1 0 0 0]\n",
            "[0 0 0 1 0 0 0]\n",
            "[0 0 0 1 0 0 0]\n",
            "[ 1 -1 -1 -1  0  0  0]\n",
            "\n",
            "\n",
            "=== Test 4: P-1 block vertical ===\n",
            "Initial board:\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 1 0 0 0]\n",
            "[0 0 0 1 0 0 0]\n",
            "[0 0 0 1 0 0 0]\n",
            "[ 0 -1 -1  1  0  0  0]\n",
            "\n",
            "Player -1 chooses column: 0\n",
            "Board after move:\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 1 0 0 0]\n",
            "[0 0 0 1 0 0 0]\n",
            "[0 0 0 1 0 0 0]\n",
            "[-1 -1 -1  1  0  0  0]\n",
            "\n",
            "\n",
            "=== Test 5: P1 diagonal win ===\n",
            "Initial board:\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 1 0 0 0]\n",
            "[ 0  0  1 -1  0  0  0]\n",
            "[ 0  1 -1 -1  0  0  0]\n",
            "[ 1 -1 -1 -1  0  0  0]\n",
            "\n",
            "Player 1 chooses column: 0\n",
            "Board after move:\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0]\n",
            "[0 0 0 1 0 0 0]\n",
            "[ 0  0  1 -1  0  0  0]\n",
            "[ 1  1 -1 -1  0  0  0]\n",
            "[ 1 -1 -1 -1  0  0  0]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def print_board(board):\n",
        "    for row in board:\n",
        "        print(row)\n",
        "    print()\n",
        "\n",
        "agent = MinimaxAgent()   # alphabeta +heuristic, depth-limited\n",
        "\n",
        "tests = [\n",
        "    (board1, player1, \"P1 horizontal win\"),\n",
        "    (board2, player2, \"P-1 block horizontal\"),\n",
        "    (board3, player3, \"P1 vertical win\"),\n",
        "    (board4, player4, \"P-1 block vertical\"),\n",
        "    (board5, player5, \"P1 diagonal win\")\n",
        "]\n",
        "\n",
        "for i, (board, player, desc) in enumerate(tests, start=1):\n",
        "\n",
        "    print(f\"\\n=== Test {i}: {desc} ===\")\n",
        "    print(\"Initial board:\")\n",
        "    print_board(board)\n",
        "\n",
        "    move = agent(board, player)   # depth cutoff included internally\n",
        "    print(f\"Player {player} chooses column: {move}\")\n",
        "\n",
        "    if move is not None:\n",
        "        new_board = result(board, player, move)\n",
        "        print(\"Board after move:\")\n",
        "        print_board(new_board)\n",
        "    else:\n",
        "        print(\"No legal move available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even with alphabeta pruning, minimax becomes slower as the board gets wider and taller because the branching factor increases. A 4Ã—4 board has very few possible actions, but a 6Ã—7 board starts with 7 legal moves and the search tree can grow enormously.\n",
        "\n",
        "A depth-4 search already explores thousands of positions.\n",
        "A full, non-cutoff minimax on a 6Ã—7 board would require exploring on the order of 7^{42} possible states, which is completely impossible to compute.\n",
        "\n",
        "So even though my depth-limited version runs reasonably fast on small boards, running full minimax on a standard 6Ã—7 Connect-4 board is not feasible."
      ],
      "metadata": {
        "id": "PNq3d_eFvYGh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3hsiEVQO926"
      },
      "source": [
        "How long does it take to make a move? Start with a smaller board with 4 columns and make the board larger by adding columns/rows. Explain why using this algorithm on a standard $6 \\times 7$ board is not feasible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8_D35hTO926",
        "outputId": "dfb691f8-b93a-4ea4-ed5a-faec5c4cc936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Measuring minimax (with depth cutoff) move time:\n",
            "\n",
            "4Ã—4 board â†’ 0.0204 seconds\n",
            "4Ã—5 board â†’ 0.0578 seconds\n",
            "5Ã—5 board â†’ 0.0796 seconds\n",
            "5Ã—6 board â†’ 0.2058 seconds\n",
            "6Ã—7 board â†’ 0.5768 seconds\n"
          ]
        }
      ],
      "source": [
        "def measure_move_time(rows, cols):\n",
        "    board = np.zeros((rows, cols), dtype=int)\n",
        "    agent = MinimaxAgent()\n",
        "\n",
        "    start = time.time()\n",
        "    _ = agent(board, player=1)\n",
        "    end = time.time()\n",
        "\n",
        "    return end - start\n",
        "\n",
        "sizes = [\n",
        "    (4,4),\n",
        "    (4,5),\n",
        "    (5,5),\n",
        "    (5,6),\n",
        "    (6,7)  # standard Connect-4\n",
        "]\n",
        "\n",
        "print(\"Measuring minimax (with depth cutoff) move time:\\n\")\n",
        "for r, c in sizes:\n",
        "    t = measure_move_time(r, c)\n",
        "    print(f\"{r}Ã—{c} board â†’ {t:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wskKaw7iO926"
      },
      "source": [
        "### Move ordering [5 points]\n",
        "\n",
        "Starting the search with better moves will increase the efficiency of alpha-beta pruning. Describe and implement a simple move ordering strategy. Make a table that shows how the ordering strategies influence the time it takes to make a move?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "jJsebEtMO926"
      },
      "outputs": [],
      "source": [
        "def ordered_actions(board):\n",
        "    \"\"\"Return legal actions sorted to try center columns first.\"\"\"\n",
        "    cols = actions(board)\n",
        "    center = board.shape[1] // 2\n",
        "    return sorted(cols, key=lambda c: abs(c - center))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Minimax With Move Ordering"
      ],
      "metadata": {
        "id": "tFSwmCsmrJxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def minimax_value_ordered(board, player, alpha, beta, depth):\n",
        "    if terminal(board) or depth == 0:\n",
        "        return heuristic(board, player)\n",
        "\n",
        "    if player == 1:  # maximizing\n",
        "        best = -math.inf\n",
        "        for col in ordered_actions(board):    # <--- ordered moves\n",
        "            child = result(board, player, col)\n",
        "            val = minimax_value_ordered(child, -player, alpha, beta, depth-1)\n",
        "            best = max(best, val)\n",
        "            alpha = max(alpha, best)\n",
        "            if beta <= alpha:\n",
        "                break\n",
        "        return best\n",
        "\n",
        "    else:  # minimizing\n",
        "        best = math.inf\n",
        "        for col in ordered_actions(board):    # <--- ordered moves\n",
        "            child = result(board, player, col)\n",
        "            val = minimax_value_ordered(child, -player, alpha, beta, depth-1)\n",
        "            best = min(best, val)\n",
        "            beta = min(beta, best)\n",
        "            if beta <= alpha:\n",
        "                break\n",
        "        return best"
      ],
      "metadata": {
        "id": "7IDNgGBGp9Aa"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def minimax_move_ordered(board, player=1, depth=4):\n",
        "    best_move = None\n",
        "\n",
        "    if player == 1:\n",
        "        best_val = -math.inf\n",
        "        for col in ordered_actions(board):\n",
        "            child = result(board, player, col)\n",
        "            val = minimax_value_ordered(child, -player, -math.inf, math.inf, depth-1)\n",
        "            if val > best_val:\n",
        "                best_val = val\n",
        "                best_move = col\n",
        "\n",
        "    else:\n",
        "        best_val = math.inf\n",
        "        for col in ordered_actions(board):\n",
        "            child = result(board, player, col)\n",
        "            val = minimax_value_ordered(child, -player, -math.inf, math.inf, depth-1)\n",
        "            if val < best_val:\n",
        "                best_val = val\n",
        "                best_move = col\n",
        "\n",
        "    return best_move"
      ],
      "metadata": {
        "id": "hyrIwE_-vm7f"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Timing Comparison (No Ordering vs Ordering)"
      ],
      "metadata": {
        "id": "9eS4xuIDrOv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def measure(agent_func, board, depth=4):\n",
        "    start = time.time()\n",
        "    agent_func(board, 1, depth)\n",
        "    return time.time() - start\n",
        "\n",
        "# Fix: pass shape tuple into empty_board\n",
        "boards_to_test = [\n",
        "    empty_board((4,4)),\n",
        "    empty_board((4,5)),\n",
        "    empty_board((5,5)),\n",
        "]\n",
        "\n",
        "print(\"Timing comparison (seconds):\\n\")\n",
        "print(\"Board\\t\\tNo Ordering\\tWith Ordering\")\n",
        "\n",
        "for b in boards_to_test:\n",
        "    t1 = measure(minimax_move, b, depth=4)\n",
        "    t2 = measure(minimax_move_ordered, b, depth=4)\n",
        "    print(f\"{b.shape}\\t{t1:.4f}\\t\\t{t2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nisi7QtRp9n6",
        "outputId": "f18a64d0-8054-4161-f05a-ab3f28a98818"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timing comparison (seconds):\n",
            "\n",
            "Board\t\tNo Ordering\tWith Ordering\n",
            "(4, 4)\t0.0112\t\t0.0330\n",
            "(4, 5)\t0.0678\t\t0.0606\n",
            "(5, 5)\t0.0780\t\t0.1108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-WvRPRpO926"
      },
      "source": [
        "### The first few moves [5 points]\n",
        "\n",
        "Start with an empty board. This is the worst case scenario for minimax search since it needs solve all possible games that can be played (minus some pruning) before making the decision. What can you do?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting from an empty board is basically the worst possible situation for minimax. At the beginning of the game, every column is available, the branching factor is huge, and the game can go all the way to 42 moves. If you try to explore the entire game tree from this position, youâ€™d be looking at something on the scale of 7^{42} possible states, which is completely impossible to search through, even with alphaâ€“beta pruning.\n",
        "\n",
        "To make the early moves actually feasible, I donâ€™t let minimax search all the way to the end. Instead, I apply a depth cutoff during the first few moves (usually depth 4â€“6) and use my heuristic evaluation function to estimate the board value when the cutoff is reached. This still lets the agent catch short-term wins or threats without blowing up the computation.\n",
        "\n",
        "I also use move ordering, where I explore center columns first. This helps alphaâ€“beta pruning cut off more of the tree and makes the search a lot faster.\n",
        "\n",
        "Putting these together depth limits, heuristics, and move ordering keeps the first few moves fast and still produces good, sensible decisions."
      ],
      "metadata": {
        "id": "jl_Givh4rvR2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJXhYxuSO926"
      },
      "source": [
        "### Playtime [5 points]\n",
        "\n",
        "Let the Minimax Search agent play a random agent on a $4 \\times 4$ board. Analyze wins, losses and draws."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CE8fCOlFO926",
        "outputId": "5ac22b6a-4368-4c89-96e7-153c8a30f3bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results over 50 games:\n",
            "{'minimax_win': 20, 'random_win': 3, 'draw': 27}\n"
          ]
        }
      ],
      "source": [
        "def random_agent(board, player):\n",
        "    legal = actions(board)\n",
        "    return np.random.choice(legal) if legal else None\n",
        "\n",
        "def play_game(agent1, agent2, rows=4, cols=4):\n",
        "    board = empty_board((rows, cols))\n",
        "    player = 1  # Minimax goes first\n",
        "\n",
        "    while True:\n",
        "        move = agent1(board, player) if player == 1 else agent2(board, player)\n",
        "        if move is None:\n",
        "            return \"draw\"\n",
        "\n",
        "        board = result(board, player, move)\n",
        "\n",
        "        if terminal(board):\n",
        "            if utility(board, 1) == 1:\n",
        "                return \"minimax_win\"\n",
        "            elif utility(board, 1) == -1:\n",
        "                return \"random_win\"\n",
        "            else:\n",
        "                return \"draw\"\n",
        "\n",
        "        player *= -1  # switch player\n",
        "\n",
        "\n",
        "# Run multiple games\n",
        "minimax = MinimaxAgent()  # depth cutoff built inside your agent\n",
        "randomA = random_agent\n",
        "\n",
        "results = {\"minimax_win\": 0, \"random_win\": 0, \"draw\": 0}\n",
        "\n",
        "N = 50  # play 50 games\n",
        "for _ in range(N):\n",
        "    outcome = play_game(minimax, randomA, rows=4, cols=4)\n",
        "    results[outcome] += 1\n",
        "\n",
        "print(f\"Results over {N} games:\")\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUlDmicdO926"
      },
      "source": [
        "## Task 4: Heuristic Alpha-Beta Tree Search\n",
        "\n",
        "### Heuristic evaluation function [15 points]\n",
        "\n",
        "Define and implement a heuristic evaluation function. Make sure that the heuristic value stays in the correct range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "znI0TZE8O926"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def heuristic(board, player=1):\n",
        "    # If the game is already over, just use the true utility\n",
        "    if terminal(board):\n",
        "        return utility(board, player)\n",
        "\n",
        "    rows, cols = board.shape\n",
        "    opp = -player\n",
        "    score = 0\n",
        "\n",
        "    def eval_window(window):\n",
        "        \"\"\"Score a 4-cell window.\"\"\"\n",
        "        window = np.array(window)\n",
        "        cnt_p = np.count_nonzero(window == player)\n",
        "        cnt_o = np.count_nonzero(window == opp)\n",
        "        cnt_0 = np.count_nonzero(window == 0)\n",
        "\n",
        "        # If both pplayers are in this window, it's kinda neutral\n",
        "        if cnt_p > 0 and cnt_o > 0:\n",
        "            return 0\n",
        "\n",
        "        s = 0\n",
        "        #Favour our potential lines\n",
        "        if cnt_p == 3 and cnt_0 == 1:\n",
        "            s += 5\n",
        "        elif cnt_p == 2 and cnt_0 == 2:\n",
        "            s += 2\n",
        "        elif cnt_p == 1 and cnt_0 == 3:\n",
        "            s += 0.5\n",
        "\n",
        "        #punish opponent's potential lines\n",
        "        if cnt_o == 3 and cnt_0 == 1:\n",
        "            s -= 5\n",
        "        elif cnt_o == 2 and cnt_0 == 2:\n",
        "            s -= 2\n",
        "        elif cnt_o == 1 and cnt_0 == 3:\n",
        "            s -= 0.5\n",
        "\n",
        "        return s\n",
        "\n",
        "    #horizontal windows\n",
        "    for r in range(rows):\n",
        "        for c in range(cols - 3):\n",
        "            window = board[r, c:c+4]\n",
        "            score += eval_window(window)\n",
        "\n",
        "    #Vertical windows\n",
        "    for c in range(cols):\n",
        "        for r in range(rows - 3):\n",
        "            window = board[r:r+4, c]\n",
        "            score += eval_window(window)\n",
        "\n",
        "    #Diagonal (down-right)\n",
        "    for r in range(rows - 3):\n",
        "        for c in range(cols - 3):\n",
        "            window = [board[r+i, c+i] for i in range(4)]\n",
        "            score += eval_window(window)\n",
        "\n",
        "    #diagonal (up-right)\n",
        "    for r in range(3, rows):\n",
        "        for c in range(cols - 3):\n",
        "            window = [board[r-i, c+i] for i in range(4)]\n",
        "            score += eval_window(window)\n",
        "\n",
        "    # Normalise to [-1, 1]. 50 is just a safe-ish cap for typical boards.\n",
        "    max_abs = 50.0\n",
        "    score = max(-max_abs, min(max_abs, score))\n",
        "    return score / max_abs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR9Tx8UMO926"
      },
      "source": [
        "### Cutting Off Search [10 points]\n",
        "\n",
        "Modify your minimax search with alpha-beta pruning to cut off search at a specified depth and use the heuristic evaluation function. Experiment with different cutoff values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "EAY4IS3TO926"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def minimax_heuristic_search(board, current_player, max_player, depth, max_depth, alpha, beta):\n",
        "    # Terminal state: use true utility\n",
        "    if terminal(board):\n",
        "        return utility(board, max_player)\n",
        "\n",
        "    # Depth cutoff: use heuristic\n",
        "    if depth == max_depth:\n",
        "        return heuristic(board, max_player)\n",
        "\n",
        "    # Maximizing for max_player, minimizing for the opponent\n",
        "    if current_player == max_player:\n",
        "        value = -math.inf\n",
        "        for col in actions(board):\n",
        "            new_board = result(board, current_player, col)\n",
        "            child_value = minimax_heuristic_search(\n",
        "                new_board,\n",
        "                -current_player,\n",
        "                max_player,\n",
        "                depth + 1,\n",
        "                max_depth,\n",
        "                alpha,\n",
        "                beta\n",
        "            )\n",
        "            value = max(value, child_value)\n",
        "            alpha = max(alpha, value)\n",
        "            if beta <= alpha:\n",
        "                break\n",
        "        return value\n",
        "    else:\n",
        "        value = math.inf\n",
        "        for col in actions(board):\n",
        "            new_board = result(board, current_player, col)\n",
        "            child_value = minimax_heuristic_search(\n",
        "                new_board,\n",
        "                -current_player,\n",
        "                max_player,\n",
        "                depth + 1,\n",
        "                max_depth,\n",
        "                alpha,\n",
        "                beta\n",
        "            )\n",
        "            value = min(value, child_value)\n",
        "            beta = min(beta, value)\n",
        "            if beta <= alpha:\n",
        "                break\n",
        "        return value\n",
        "\n",
        "\n",
        "class HeuristicMinimaxAgent:\n",
        "    def __init__(self, max_depth=4):\n",
        "        # how far we search before using the heuristic\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def __call__(self, board, player=1):\n",
        "\n",
        "        return self.make_move(board, player)\n",
        "\n",
        "    def make_move(self, board, player=1):\n",
        "        max_player = player\n",
        "        best_move = None\n",
        "\n",
        "        # If no legal moves, return None\n",
        "        legal_moves = actions(board)\n",
        "        if not legal_moves:\n",
        "            return None\n",
        "\n",
        "        # Root alpha-beta loop\n",
        "        if max_player == 1:\n",
        "            best_value = -math.inf\n",
        "            alpha, beta = -math.inf, math.inf\n",
        "            for col in legal_moves:\n",
        "                new_board = result(board, max_player, col)\n",
        "                value = minimax_heuristic_search(\n",
        "                    new_board,\n",
        "                    -max_player,\n",
        "                    max_player,\n",
        "                    depth=1,\n",
        "                    max_depth=self.max_depth,\n",
        "                    alpha=alpha,\n",
        "                    beta=beta\n",
        "                )\n",
        "                if value > best_value:\n",
        "                    best_value = value\n",
        "                    best_move = col\n",
        "                alpha = max(alpha, best_value)\n",
        "        else:\n",
        "            best_value = math.inf\n",
        "            alpha, beta = -math.inf, math.inf\n",
        "            for col in legal_moves:\n",
        "                new_board = result(board, max_player, col)\n",
        "                value = minimax_heuristic_search(\n",
        "                    new_board,\n",
        "                    -max_player,\n",
        "                    max_player,\n",
        "                    depth=1,\n",
        "                    max_depth=self.max_depth,\n",
        "                    alpha=alpha,\n",
        "                    beta=beta\n",
        "                )\n",
        "                if value < best_value:\n",
        "                    best_value = value\n",
        "                    best_move = col\n",
        "                beta = min(beta, best_value)\n",
        "\n",
        "        return best_move"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz7KFO9RO926"
      },
      "source": [
        "Experiment with the same manually created boards as above to check if the agent spots wining opportunities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9XtzMHPO926",
        "outputId": "7737b016-8fda-4b42-aaeb-819ad3aadebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Scenario 1 ===\n",
            "P1 should win horizontally\n",
            "Player to move: 1\n",
            "Initial board:\n",
            "[[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 1 1 1 0 0 0]]\n",
            "Heuristic agent chooses column: 0\n",
            "Resulting board:\n",
            "[[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [1 1 1 1 0 0 0]]\n",
            "\n",
            "=== Scenario 2 ===\n",
            "P-1 should block P1 horizontal win\n",
            "Player to move: -1\n",
            "Initial board:\n",
            "[[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 1 1 1 0 0 0]]\n",
            "Heuristic agent chooses column: 0\n",
            "Resulting board:\n",
            "[[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [-1  1  1  1  0  0  0]]\n",
            "\n",
            "=== Scenario 3 ===\n",
            "P1 should win vertically\n",
            "Player to move: 1\n",
            "Initial board:\n",
            "[[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0 -1 -1 -1  0  0  0]]\n",
            "Heuristic agent chooses column: 3\n",
            "Resulting board:\n",
            "[[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0 -1 -1 -1  0  0  0]]\n",
            "\n",
            "=== Scenario 4 ===\n",
            "P-1 should block P1 vertical win\n",
            "Player to move: -1\n",
            "Initial board:\n",
            "[[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0 -1 -1  1  0  0  0]]\n",
            "Heuristic agent chooses column: 0\n",
            "Resulting board:\n",
            "[[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [-1 -1 -1  1  0  0  0]]\n",
            "\n",
            "=== Scenario 5 ===\n",
            "P1 should win on diagonal\n",
            "Player to move: 1\n",
            "Initial board:\n",
            "[[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  1 -1  0  0  0]\n",
            " [ 0  1 -1 -1  0  0  0]\n",
            " [ 1 -1 -1 -1  0  0  0]]\n",
            "Heuristic agent chooses column: 0\n",
            "Resulting board:\n",
            "[[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  1 -1  0  0  0]\n",
            " [ 1  1 -1 -1  0  0  0]\n",
            " [ 1 -1 -1 -1  0  0  0]]\n",
            "\n",
            "Heuristic agent tests done.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "heur_agent = HeuristicMinimaxAgent(max_depth=4)\n",
        "\n",
        "scenarios = [\n",
        "    (test_board_1,  1, \"P1 should win horizontally\"),\n",
        "    (test_board_2, -1, \"P-1 should block P1 horizontal win\"),\n",
        "    (test_board_3,  1, \"P1 should win vertically\"),\n",
        "    (test_board_4, -1, \"P-1 should block P1 vertical win\"),\n",
        "    (test_board_5,  1, \"P1 should win on diagonal\"),\n",
        "]\n",
        "\n",
        "for i, (board, player, note) in enumerate(scenarios, start=1):\n",
        "    print(f\"\\n=== Scenario {i} ===\")\n",
        "    print(note)\n",
        "    print(f\"Player to move: {player}\")\n",
        "    print(\"Initial board:\")\n",
        "    print(board)\n",
        "\n",
        "    move = heur_agent(board, player)\n",
        "    print(f\"Heuristic agent chooses column: {move}\")\n",
        "\n",
        "    if move is not None:\n",
        "        new_board = result(board, player, move)\n",
        "        print(\"Resulting board:\")\n",
        "        print(new_board)\n",
        "    else:\n",
        "        print(\"No legal move found (probably full column/board).\")\n",
        "\n",
        "print(\"\\nHeuristic agent tests done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BdwX2i2O926"
      },
      "source": [
        "How long does it take to make a move? Start with a smaller board with 4 columns and make the board larger by adding columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rab461aBO926",
        "outputId": "d81447b5-9fe0-4dda-9697-a9ba9e8bd886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Heuristic minimax, max_depth = 4\n",
            "Average time per move on empty boards:\n",
            "\n",
            "Board 6x4: 0.0190 seconds per move\n",
            "Board 6x5: 0.1014 seconds per move\n",
            "Board 6x6: 0.2974 seconds per move\n",
            "Board 6x7: 0.6510 seconds per move\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "heur_agent = HeuristicMinimaxAgent(max_depth=4)\n",
        "\n",
        "def time_one_move(board_shape, player=1, trials=3):\n",
        "    \"\"\"Rough timing for one heuristic minimax move on an empty board.\"\"\"\n",
        "    total = 0.0\n",
        "    for _ in range(trials):\n",
        "        board = empty_board(board_shape)\n",
        "        start = time.perf_counter()\n",
        "        _ = heur_agent(board, player)\n",
        "        end = time.perf_counter()\n",
        "        total += (end - start)\n",
        "    return total / trials\n",
        "\n",
        "# start with 4 columns and increase\n",
        "board_shapes = [\n",
        "    (6, 4),  # 6 rows, 4 columns\n",
        "    (6, 5),\n",
        "    (6, 6),\n",
        "    (6, 7),  # standard width\n",
        "]\n",
        "\n",
        "print(f\"Heuristic minimax, max_depth = {heur_agent.max_depth}\")\n",
        "print(\"Average time per move on empty boards:\\n\")\n",
        "\n",
        "for shape in board_shapes:\n",
        "    avg_t = time_one_move(shape, player=1, trials=3)\n",
        "    print(f\"Board {shape[0]}x{shape[1]}: {avg_t:.4f} seconds per move\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we add more columns, the branching factor grows (there are more legal actions per state), so the number of nodes explored by minimax with depth d grows roughly like b^d. Thatâ€™s why even with a fixed cutoff depth the move time goes up as we widen the board.\n",
        "\n",
        "On a full 6Ã—7 board with a larger depth limit the runtime would blow up really fast, so using a cutoff depth (and a heuristic eval) is kind of necessary to keep the agent playable in real time."
      ],
      "metadata": {
        "id": "7J5DqdgHoFfP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgsDMwEOO926"
      },
      "source": [
        "### Playtime [5 points]\n",
        "\n",
        "Let two heuristic search agents (different cutoff depth) compete against each other on a reasonably sized board. Since there is no randomness, you only need to let them play once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dghExatdO927",
        "outputId": "77d19f1f-d332-4ad4-abfb-3dc50c4651ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Heuristic vs Heuristic on 6x7 ---\n",
            "Initial board:\n",
            " [[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]]\n",
            "\n",
            "Turn 1: player 1 plays column 2\n",
            "[[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0]]\n",
            "\n",
            "Turn 2: player -1 plays column 6\n",
            "[[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  1  0  0  0 -1]]\n",
            "\n",
            "Turn 3: player 1 plays column 3\n",
            "[[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  1  1  0  0 -1]]\n",
            "\n",
            "Turn 4: player -1 plays column 0\n",
            "[[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [-1  0  1  1  0  0 -1]]\n",
            "\n",
            "Turn 5: player 1 plays column 4\n",
            "[[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [-1  0  1  1  1  0 -1]]\n",
            "\n",
            "Turn 6: player -1 plays column 0\n",
            "[[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [-1  0  0  0  0  0  0]\n",
            " [-1  0  1  1  1  0 -1]]\n",
            "\n",
            "Turn 7: player 1 plays column 0\n",
            "[[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 1  0  0  0  0  0  0]\n",
            " [-1  0  0  0  0  0  0]\n",
            " [-1  0  1  1  1  0 -1]]\n",
            "\n",
            "Turn 8: player -1 plays column 0\n",
            "[[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [-1  0  0  0  0  0  0]\n",
            " [ 1  0  0  0  0  0  0]\n",
            " [-1  0  0  0  0  0  0]\n",
            " [-1  0  1  1  1  0 -1]]\n",
            "\n",
            "Turn 9: player 1 plays column 0\n",
            "[[ 0  0  0  0  0  0  0]\n",
            " [ 1  0  0  0  0  0  0]\n",
            " [-1  0  0  0  0  0  0]\n",
            " [ 1  0  0  0  0  0  0]\n",
            " [-1  0  0  0  0  0  0]\n",
            " [-1  0  1  1  1  0 -1]]\n",
            "\n",
            "Turn 10: player -1 plays column 0\n",
            "[[-1  0  0  0  0  0  0]\n",
            " [ 1  0  0  0  0  0  0]\n",
            " [-1  0  0  0  0  0  0]\n",
            " [ 1  0  0  0  0  0  0]\n",
            " [-1  0  0  0  0  0  0]\n",
            " [-1  0  1  1  1  0 -1]]\n",
            "\n",
            "Turn 11: player 1 plays column 1\n",
            "[[-1  0  0  0  0  0  0]\n",
            " [ 1  0  0  0  0  0  0]\n",
            " [-1  0  0  0  0  0  0]\n",
            " [ 1  0  0  0  0  0  0]\n",
            " [-1  0  0  0  0  0  0]\n",
            " [-1  1  1  1  1  0 -1]]\n",
            "\n",
            "Game over: player 1 wins.\n",
            "\n",
            "Result: player 1 (depth 3) wins.\n"
          ]
        }
      ],
      "source": [
        "# Your code/ answer goes here.# Playtime: heuristic vs heuristic with different depths\n",
        "\n",
        "def play_game_heuristic(agent1, agent2, board_shape=(6, 7), verbose=True):\n",
        "    \"\"\"Let two heuristic agents play one game on a given board size.\"\"\"\n",
        "    board = empty_board(board_shape)\n",
        "    current_player = 1   # player 1 starts\n",
        "    turn = 0\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n--- Heuristic vs Heuristic on {board_shape[0]}x{board_shape[1]} ---\")\n",
        "        print(\"Initial board:\\n\", board)\n",
        "\n",
        "    while not terminal(board):\n",
        "        turn += 1\n",
        "\n",
        "        if current_player == 1:\n",
        "            action = agent1(board, current_player)\n",
        "        else:\n",
        "            action = agent2(board, current_player)\n",
        "\n",
        "        if action is None:\n",
        "            # no legal moves -> draw (should only happen on full board)\n",
        "            if verbose:\n",
        "                print(f\"Player {current_player} has no legal moves. Draw.\")\n",
        "            return 0\n",
        "\n",
        "        board = result(board, current_player, action)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nTurn {turn}: player {current_player} plays column {action}\")\n",
        "            print(board)\n",
        "\n",
        "        if check_win(board, current_player):\n",
        "            if verbose:\n",
        "                print(f\"\\nGame over: player {current_player} wins.\")\n",
        "            return current_player\n",
        "\n",
        "        # switch player\n",
        "        current_player *= -1\n",
        "\n",
        "    # full board, no winner\n",
        "    if verbose:\n",
        "        print(\"\\nGame over: draw (board full).\")\n",
        "    return 0\n",
        "\n",
        "\n",
        "# two heuristic agents with different cutoff depths\n",
        "agent_depth3 = HeuristicMinimaxAgent(max_depth=3)\n",
        "agent_depth5 = HeuristicMinimaxAgent(max_depth=5)\n",
        "\n",
        "# let them play once on a \"reasonable\" board (6x7 standard)\n",
        "winner = play_game_heuristic(agent_depth3, agent_depth5, board_shape=(6, 7), verbose=True)\n",
        "\n",
        "if winner == 1:\n",
        "    print(\"\\nResult: player 1 (depth 3) wins.\")\n",
        "elif winner == -1:\n",
        "    print(\"\\nResult: player -1 (depth 5) wins.\")\n",
        "else:\n",
        "    print(\"\\nResult: draw.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we add more columns, the branching factor grows (there are more legal actions per state), so the number of nodes explored by minimax with depth d grows roughly like b^d. Thatâ€™s why even with a fixed cutoff depth the move time goes up as we widen the board.\n",
        "\n",
        "On a full 6Ã—7 board with a larger depth limit the runtime would blow up really fast, so using a cutoff depth (and a heuristic eval) is kind of necessary to keep the agent playable in real time."
      ],
      "metadata": {
        "id": "g6F2GnFgoJF8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZfxMKr6O927"
      },
      "source": [
        "## Graduate student advanced task: Pure Monte Carlo Search and Best First Move [10 point]\n",
        "\n",
        "__Undergraduate students:__ This is a bonus task you can attempt if you like [+5 bonus point].\n",
        "\n",
        "### Pure Monte Carlo Search\n",
        "\n",
        "Implement Pure Monte Carlo Search and investigate how this search performs on the test boards that you have used above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "zInQBDyYO927"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "def random_rollout(board, current_player):\n",
        "    \"\"\"\n",
        "    Play out a game to the end with random moves.\n",
        "    Returns utility from the perspective of the player who started the rollout.\n",
        "    \"\"\"\n",
        "    root_player = current_player\n",
        "    temp_board = board.copy()\n",
        "\n",
        "    while not terminal(temp_board):\n",
        "        valid_moves = actions(temp_board)\n",
        "        if not valid_moves:\n",
        "            break\n",
        "        move = random.choice(valid_moves)\n",
        "        temp_board = result(temp_board, current_player, move)\n",
        "        current_player *= -1  # switch player\n",
        "\n",
        "    return utility(temp_board, root_player)\n",
        "\n",
        "\n",
        "def monte_carlo_move(board, player, num_rollouts=200):\n",
        "    \"\"\"\n",
        "    Pure Monte Carlo: for each legal move, run random rollouts and pick the move\n",
        "    with the best average utility for `player`.\n",
        "    \"\"\"\n",
        "    valid_moves = actions(board)\n",
        "    if not valid_moves:\n",
        "        return None\n",
        "\n",
        "    best_move = None\n",
        "    best_score = -math.inf\n",
        "\n",
        "    for move in valid_moves:\n",
        "        next_board = result(board, player, move)\n",
        "        # if this move already wins, just take it\n",
        "        if check_win(next_board, player):\n",
        "            return move\n",
        "\n",
        "        total_score = 0\n",
        "        for _ in range(num_rollouts):\n",
        "            total_score += random_rollout(next_board, -player) * -1  # flip back to player's view\n",
        "\n",
        "        avg_score = total_score / num_rollouts\n",
        "\n",
        "        if avg_score > best_score:\n",
        "            best_score = avg_score\n",
        "            best_move = move\n",
        "\n",
        "    return best_move\n",
        "\n",
        "\n",
        "class MonteCarloAgent:\n",
        "    def __init__(self, num_rollouts=200):\n",
        "        self.num_rollouts = num_rollouts\n",
        "\n",
        "    def __call__(self, board, player=1):\n",
        "        # keep the same signature as random_player / minimax agents\n",
        "        return monte_carlo_move(board, player, self.num_rollouts)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mc_agent = MonteCarloAgent(num_rollouts=300)\n",
        "\n",
        "test_positions = [\n",
        "    (\"Scenario 1: P1 win horizontal\",    test_board_1,  1),\n",
        "    (\"Scenario 2: P-1 block horizontal\", test_board_2, -1),\n",
        "    (\"Scenario 3: P1 win vertical\",      test_board_3,  1),\n",
        "    (\"Scenario 4: P-1 block vertical\",   test_board_4, -1),\n",
        "    (\"Scenario 5: P1 win diagonal\",      test_board_5,  1),\n",
        "]\n",
        "\n",
        "for name, board, current_player in test_positions:\n",
        "    print(\"\\n\" + name)\n",
        "    print(\"Current player:\", current_player)\n",
        "    print(\"Board:\\n\", board)\n",
        "\n",
        "    move = mc_agent(board, current_player)\n",
        "    print(\"Monte Carlo chosen move (column):\", move)\n",
        "\n",
        "    if move is not None:\n",
        "        new_board = result(board, current_player, move)\n",
        "        print(\"Resulting board:\\n\", new_board)\n",
        "        if check_win(new_board, current_player):\n",
        "            print(\"â†’ This move wins for player\", current_player)\n",
        "        else:\n",
        "            print(\"â†’ This move does not win immediately (likely a blocking / setup move).\")\n",
        "    else:\n",
        "        print(\"No legal move found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoel6KIwoP0s",
        "outputId": "a2a88d07-d80c-4d79-a866-96827c6cff2a"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scenario 1: P1 win horizontal\n",
            "Current player: 1\n",
            "Board:\n",
            " [[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 1 1 1 0 0 0]]\n",
            "Monte Carlo chosen move (column): 0\n",
            "Resulting board:\n",
            " [[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [1 1 1 1 0 0 0]]\n",
            "â†’ This move wins for player 1\n",
            "\n",
            "Scenario 2: P-1 block horizontal\n",
            "Current player: -1\n",
            "Board:\n",
            " [[0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0]\n",
            " [0 1 1 1 0 0 0]]\n",
            "Monte Carlo chosen move (column): 4\n",
            "Resulting board:\n",
            " [[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  1  1  1 -1  0  0]]\n",
            "â†’ This move does not win immediately (likely a blocking / setup move).\n",
            "\n",
            "Scenario 3: P1 win vertical\n",
            "Current player: 1\n",
            "Board:\n",
            " [[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0 -1 -1 -1  0  0  0]]\n",
            "Monte Carlo chosen move (column): 3\n",
            "Resulting board:\n",
            " [[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0 -1 -1 -1  0  0  0]]\n",
            "â†’ This move wins for player 1\n",
            "\n",
            "Scenario 4: P-1 block vertical\n",
            "Current player: -1\n",
            "Board:\n",
            " [[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0 -1 -1  1  0  0  0]]\n",
            "Monte Carlo chosen move (column): 3\n",
            "Resulting board:\n",
            " [[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0 -1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0 -1 -1  1  0  0  0]]\n",
            "â†’ This move does not win immediately (likely a blocking / setup move).\n",
            "\n",
            "Scenario 5: P1 win diagonal\n",
            "Current player: 1\n",
            "Board:\n",
            " [[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  1 -1  0  0  0]\n",
            " [ 0  1 -1 -1  0  0  0]\n",
            " [ 1 -1 -1 -1  0  0  0]]\n",
            "Monte Carlo chosen move (column): 0\n",
            "Resulting board:\n",
            " [[ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0]\n",
            " [ 0  0  0  1  0  0  0]\n",
            " [ 0  0  1 -1  0  0  0]\n",
            " [ 1  1 -1 -1  0  0  0]\n",
            " [ 1 -1 -1 -1  0  0  0]]\n",
            "â†’ This move wins for player 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pure Monte Carlo Search\n",
        "I implemented a Monte Carlo agent that, for each legal move, runs a fixed number of random rollouts until a terminal state and uses the average utility as an estimate of move quality. The agent then selects the move with the highest estimated value for the current player.\n",
        "\n",
        "With around 300 rollouts per move, the Monte Carlo agent consistently:\n",
        "\t\t-Chose the immediate winning move in positions 1, 3, and 5.\n",
        "\t-Chose the correct blocking move in positions 2 and 4, preventing the opponentâ€™s next-move win\n",
        "\n",
        "Because the rollouts are random, the result can occasionally be noisy for low rollout counts, but with a few hundred rollouts the behavior was stable. Pure Monte Carlo does not look ahead in a structured way like minimax; it just estimates move quality by simulation. It still performs surprisingly well on â€œtacticalâ€ test boards where wins and blocks have a strong impact on the rollout outcomes."
      ],
      "metadata": {
        "id": "48VO3vQxoTDc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBK5H1xVO927"
      },
      "source": [
        "### Best First Move\n",
        "\n",
        "Use your Monte Carlo Search to determine what the best first move for red is? Describe under what assumptions this is the \"best\" first move.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhqX-3DWO927",
        "outputId": "8fd56928-636f-455d-f30b-52cbbb080a9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Monte Carlo value for each first move (player 1 = Red):\n",
            "  column 0 -> 0.070\n",
            "  column 1 -> 0.100\n",
            "  column 2 -> 0.185\n",
            "  column 3 -> 0.247\n",
            "  column 4 -> 0.115\n",
            "  column 5 -> 0.094\n",
            "  column 6 -> 0.004\n",
            "\n",
            "Best first move for Red according to Monte Carlo: column 3\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def estimate_first_move_values(num_rollouts=500):\n",
        "    board = empty_board()  # standard 6x7 empty board\n",
        "    first_player = 1\n",
        "    move_values = {}\n",
        "\n",
        "    for move in actions(board):\n",
        "        next_board = result(board, first_player, move)\n",
        "\n",
        "        total_score = 0\n",
        "        for _ in range(num_rollouts):\n",
        "\n",
        "            total_score += random_rollout(next_board, -first_player) * -1\n",
        "\n",
        "        avg_score = total_score / num_rollouts\n",
        "        move_values[move] = avg_score\n",
        "\n",
        "    return move_values\n",
        "\n",
        "# Run the experiment\n",
        "num_rollouts = 800\n",
        "first_move_scores = estimate_first_move_values(num_rollouts)\n",
        "\n",
        "print(\"Average Monte Carlo value for each first move (player 1 = Red):\")\n",
        "for col, val in sorted(first_move_scores.items()):\n",
        "    print(f\"  column {col} -> {val:.3f}\")\n",
        "\n",
        "best_first_move = max(first_move_scores, key=first_move_scores.get)\n",
        "print(f\"\\nBest first move for Red according to Monte Carlo: column {best_first_move}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS4pEDAuO927"
      },
      "source": [
        "## Competition task [extra course bonus will be awarded separately]\n",
        "\n",
        "After you have implemented you agents, find another student and let your best agent play against the other student's best player. Check the Canvas Module called Competition, there you will find a page that explains the rules, how to compete in the tournament, and what the bonus points are. You can start the tournament once you have a working agent. the tournament will continue after the assignment submission deadline. Tournament deadlines can be found on Canvas."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}